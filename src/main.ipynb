{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from finetune import Resnet18Finetuner, Resnet18FintunerForCaseD\n",
    "from dataloader import load_augmented_data\n",
    "import matplotlib.pyplot as plt\n",
    "from trainer import Trainer\n",
    "from torch import optim, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "finetuner_base = Resnet18Finetuner(pretrained=True, feature_extract=True)\n",
    "finetuner_A = Resnet18Finetuner(pretrained=True, feature_extract=True)\n",
    "finetuner_B = Resnet18Finetuner(pretrained=True, feature_extract=False)  # make all conv blocks trainable\n",
    "finetuner_C = Resnet18FintunerForCaseD(pretrained=True, feature_extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (1): BasicBlock(\n    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuner_base.model.layer1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"../face_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set hyperparameters and share with all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 64\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "lr = 0.002\n",
    "num_workers = 4  # how many subprocesses to use for data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'train': <torch.utils.data.dataloader.DataLoader at 0x173ec613070>,\n 'test': <torch.utils.data.dataloader.DataLoader at 0x173ec6131f0>}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders_dict = load_augmented_data(input_size, batch_size, data_dir, num_workers)\n",
    "dataloaders_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Case A: Re-train the softmax layer (Baseline model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When finetuning all resnet layers, feature_extract = False, the model is finetuned and all model parameters are updated.  \n",
    "If feature_extract = True, only the last layer parameters are updated, the others remain fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=100, bias=True)\n  (batch_norm): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (classifier): Softmax(dim=None)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuner_base.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "\t batch_norm.weight\n",
      "\t batch_norm.bias\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 4.8311 Acc: 0.0140\n",
      "test Loss: 4.6219 Acc: 0.0220\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 4.6177 Acc: 0.0188\n",
      "test Loss: 4.5748 Acc: 0.0200\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 4.5095 Acc: 0.0383\n",
      "test Loss: 4.4646 Acc: 0.0360\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 4.4534 Acc: 0.0460\n",
      "test Loss: 4.4074 Acc: 0.0630\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 4.3987 Acc: 0.0540\n",
      "test Loss: 4.2781 Acc: 0.0670\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 4.3118 Acc: 0.0593\n",
      "test Loss: 4.2258 Acc: 0.0790\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 4.2124 Acc: 0.0858\n",
      "test Loss: 4.1897 Acc: 0.0690\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 4.2049 Acc: 0.0850\n",
      "test Loss: 4.1475 Acc: 0.0810\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 4.1756 Acc: 0.0918\n",
      "test Loss: 4.0682 Acc: 0.1150\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 4.1331 Acc: 0.0990\n",
      "test Loss: 4.0427 Acc: 0.1150\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 4.1934 Acc: 0.0908\n",
      "test Loss: 4.1276 Acc: 0.0790\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 4.1457 Acc: 0.1008\n",
      "test Loss: 4.0860 Acc: 0.0930\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 4.1298 Acc: 0.0933\n",
      "test Loss: 4.0058 Acc: 0.1090\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 4.0554 Acc: 0.1133\n",
      "test Loss: 3.9989 Acc: 0.1190\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 4.0151 Acc: 0.1153\n",
      "test Loss: 3.9625 Acc: 0.1210\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 4.0146 Acc: 0.1150\n",
      "test Loss: 3.9294 Acc: 0.1140\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 3.9596 Acc: 0.1318\n",
      "test Loss: 3.9026 Acc: 0.1140\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 3.9436 Acc: 0.1268\n",
      "test Loss: 3.8588 Acc: 0.1490\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 3.9264 Acc: 0.1353\n",
      "test Loss: 3.8398 Acc: 0.1300\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 3.8955 Acc: 0.1328\n",
      "test Loss: 3.7870 Acc: 0.1630\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 3.9469 Acc: 0.1278\n",
      "test Loss: 3.8827 Acc: 0.1260\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 3.9454 Acc: 0.1320\n",
      "test Loss: 3.8845 Acc: 0.1280\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 3.9363 Acc: 0.1285\n",
      "test Loss: 3.8221 Acc: 0.1560\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 3.8678 Acc: 0.1463\n",
      "test Loss: 3.8018 Acc: 0.1430\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 3.8520 Acc: 0.1475\n",
      "test Loss: 3.8135 Acc: 0.1420\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 3.8626 Acc: 0.1383\n",
      "test Loss: 3.7383 Acc: 0.1710\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 3.8102 Acc: 0.1530\n",
      "test Loss: 3.7317 Acc: 0.1650\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 3.7832 Acc: 0.1665\n",
      "test Loss: 3.7403 Acc: 0.1510\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 3.7885 Acc: 0.1663\n",
      "test Loss: 3.7009 Acc: 0.1670\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 3.8001 Acc: 0.1565\n",
      "test Loss: 3.6586 Acc: 0.1800\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 3.8439 Acc: 0.1463\n",
      "test Loss: 3.7064 Acc: 0.1630\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 3.8620 Acc: 0.1435\n",
      "test Loss: 3.7703 Acc: 0.1560\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 3.7877 Acc: 0.1575\n",
      "test Loss: 3.7079 Acc: 0.1810\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 3.8477 Acc: 0.1388\n",
      "test Loss: 3.6839 Acc: 0.1660\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 3.7736 Acc: 0.1623\n",
      "test Loss: 3.7238 Acc: 0.1560\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 3.7677 Acc: 0.1605\n",
      "test Loss: 3.6346 Acc: 0.1690\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 3.7485 Acc: 0.1653\n",
      "test Loss: 3.6366 Acc: 0.1680\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 3.7391 Acc: 0.1640\n",
      "test Loss: 3.6834 Acc: 0.1870\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 3.7047 Acc: 0.1685\n",
      "test Loss: 3.5959 Acc: 0.1970\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 3.6930 Acc: 0.1695\n",
      "test Loss: 3.5973 Acc: 0.2040\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 3.7652 Acc: 0.1553\n",
      "test Loss: 3.7513 Acc: 0.1550\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 3.7648 Acc: 0.1643\n",
      "test Loss: 3.6818 Acc: 0.1760\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 3.7908 Acc: 0.1550\n",
      "test Loss: 3.6302 Acc: 0.1880\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 3.7327 Acc: 0.1588\n",
      "test Loss: 3.6608 Acc: 0.1700\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 3.7293 Acc: 0.1693\n",
      "test Loss: 3.6047 Acc: 0.1860\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 3.7015 Acc: 0.1695\n",
      "test Loss: 3.6252 Acc: 0.1790\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 3.6755 Acc: 0.1753\n",
      "test Loss: 3.5597 Acc: 0.2050\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 3.6781 Acc: 0.1785\n",
      "test Loss: 3.5566 Acc: 0.2040\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 3.6446 Acc: 0.1885\n",
      "test Loss: 3.5377 Acc: 0.2150\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 3.6179 Acc: 0.1848\n",
      "test Loss: 3.5230 Acc: 0.2120\n",
      "\n",
      "Training complete in 8m 26s\n",
      "Best test Acc: 0.215000\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "trainer_base = Trainer(finetuner_base)\n",
    "fine_tuned_model_base, hist_base = trainer_base.fit(\n",
    "    dataloaders=dataloaders_dict,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    optimizer=optim.Adam,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "train_loss_hist_base, train_acc_hist_base, test_loss_hist_base, test_acc_hist_base = hist_base"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case B: Fine tune Conv5_x and freeze the rest of Conv blocks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Unfreeze layer4 in Resnet18\n",
    "finetuner_A.unfreeze_layer(\"layer4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "\t batch_norm.weight\n",
      "\t batch_norm.bias\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 4.7991 Acc: 0.0118\n",
      "test Loss: 4.5914 Acc: 0.0190\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 4.6453 Acc: 0.0245\n",
      "test Loss: 4.5337 Acc: 0.0390\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 4.5377 Acc: 0.0390\n",
      "test Loss: 4.3956 Acc: 0.0480\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 4.4407 Acc: 0.0443\n",
      "test Loss: 4.2939 Acc: 0.0770\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 4.3715 Acc: 0.0588\n",
      "test Loss: 4.2223 Acc: 0.0930\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 4.3228 Acc: 0.0675\n",
      "test Loss: 4.1641 Acc: 0.0800\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 4.2493 Acc: 0.0803\n",
      "test Loss: 4.0809 Acc: 0.0930\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 4.2062 Acc: 0.0863\n",
      "test Loss: 4.0860 Acc: 0.0940\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 4.1669 Acc: 0.0930\n",
      "test Loss: 4.0011 Acc: 0.1240\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 4.1229 Acc: 0.1033\n",
      "test Loss: 3.9549 Acc: 0.1330\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 4.1828 Acc: 0.0880\n",
      "test Loss: 4.0104 Acc: 0.1090\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 4.1420 Acc: 0.0975\n",
      "test Loss: 3.9751 Acc: 0.1110\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 4.0859 Acc: 0.1030\n",
      "test Loss: 3.9741 Acc: 0.1200\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 4.0822 Acc: 0.0983\n",
      "test Loss: 3.9012 Acc: 0.1290\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 4.0006 Acc: 0.1245\n",
      "test Loss: 3.8423 Acc: 0.1560\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 3.9746 Acc: 0.1223\n",
      "test Loss: 3.8129 Acc: 0.1580\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 3.9753 Acc: 0.1278\n",
      "test Loss: 3.7624 Acc: 0.1590\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 3.9210 Acc: 0.1323\n",
      "test Loss: 3.8073 Acc: 0.1690\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 3.9158 Acc: 0.1365\n",
      "test Loss: 3.7410 Acc: 0.1590\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 3.9062 Acc: 0.1320\n",
      "test Loss: 3.7534 Acc: 0.1520\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 3.9488 Acc: 0.1268\n",
      "test Loss: 3.7459 Acc: 0.1580\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 3.9366 Acc: 0.1278\n",
      "test Loss: 3.7687 Acc: 0.1510\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 3.9019 Acc: 0.1400\n",
      "test Loss: 3.7057 Acc: 0.1750\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 3.8696 Acc: 0.1518\n",
      "test Loss: 3.6836 Acc: 0.1590\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 3.8880 Acc: 0.1455\n",
      "test Loss: 3.6912 Acc: 0.1470\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 3.8694 Acc: 0.1443\n",
      "test Loss: 3.7006 Acc: 0.1620\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 3.8731 Acc: 0.1470\n",
      "test Loss: 3.6746 Acc: 0.1710\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 3.8136 Acc: 0.1565\n",
      "test Loss: 3.5841 Acc: 0.1830\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 3.7893 Acc: 0.1533\n",
      "test Loss: 3.5841 Acc: 0.2120\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 3.7899 Acc: 0.1600\n",
      "test Loss: 3.5853 Acc: 0.1970\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 3.8367 Acc: 0.1428\n",
      "test Loss: 3.6501 Acc: 0.1920\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 3.8493 Acc: 0.1465\n",
      "test Loss: 3.6477 Acc: 0.1800\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 3.8179 Acc: 0.1485\n",
      "test Loss: 3.6350 Acc: 0.1710\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 3.8127 Acc: 0.1525\n",
      "test Loss: 3.6189 Acc: 0.1930\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 3.7908 Acc: 0.1610\n",
      "test Loss: 3.5655 Acc: 0.1850\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 3.7703 Acc: 0.1558\n",
      "test Loss: 3.5647 Acc: 0.2090\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 3.7067 Acc: 0.1675\n",
      "test Loss: 3.5584 Acc: 0.1960\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 3.6939 Acc: 0.1710\n",
      "test Loss: 3.5262 Acc: 0.1990\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 3.6947 Acc: 0.1775\n",
      "test Loss: 3.4869 Acc: 0.2270\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 3.7222 Acc: 0.1750\n",
      "test Loss: 3.4891 Acc: 0.1990\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 3.7069 Acc: 0.1708\n",
      "test Loss: 3.5299 Acc: 0.2290\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 3.7307 Acc: 0.1638\n",
      "test Loss: 3.5351 Acc: 0.2200\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 3.7516 Acc: 0.1660\n",
      "test Loss: 3.5910 Acc: 0.1960\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 3.7161 Acc: 0.1733\n",
      "test Loss: 3.5011 Acc: 0.2110\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 3.7037 Acc: 0.1738\n",
      "test Loss: 3.5087 Acc: 0.2080\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 3.7305 Acc: 0.1728\n",
      "test Loss: 3.5309 Acc: 0.2130\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 3.6906 Acc: 0.1758\n",
      "test Loss: 3.4926 Acc: 0.2160\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 3.6560 Acc: 0.1890\n",
      "test Loss: 3.4661 Acc: 0.2230\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 3.6872 Acc: 0.1818\n",
      "test Loss: 3.4381 Acc: 0.2270\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 3.6589 Acc: 0.1798\n",
      "test Loss: 3.4059 Acc: 0.2380\n",
      "\n",
      "Training complete in 8m 16s\n",
      "Best test Acc: 0.238000\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "trainer_A = Trainer(finetuner_A)\n",
    "fine_tuned_model_A, hist_A = trainer_A.fit(\n",
    "    dataloaders=dataloaders_dict,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    optimizer=optim.Adam,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "train_loss_hist_A, train_acc_hist_A, test_loss_hist_A, test_acc_hist_A = hist_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Case C: Fine tune ALL convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "\t batch_norm.weight\n",
      "\t batch_norm.bias\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 4.5661 Acc: 0.0293\n",
      "test Loss: 5.6880 Acc: 0.0410\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 4.0730 Acc: 0.0595\n",
      "test Loss: 4.8406 Acc: 0.0350\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 3.7677 Acc: 0.0950\n",
      "test Loss: 3.8143 Acc: 0.0870\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 3.4619 Acc: 0.1385\n",
      "test Loss: 3.2963 Acc: 0.1830\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 3.2003 Acc: 0.1903\n",
      "test Loss: 3.1324 Acc: 0.2050\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 2.8514 Acc: 0.2635\n",
      "test Loss: 2.3651 Acc: 0.3460\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 2.5934 Acc: 0.3230\n",
      "test Loss: 2.3146 Acc: 0.3610\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 2.4267 Acc: 0.3640\n",
      "test Loss: 2.9600 Acc: 0.2510\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 2.2079 Acc: 0.4165\n",
      "test Loss: 2.1191 Acc: 0.4520\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 2.0348 Acc: 0.4673\n",
      "test Loss: 1.8218 Acc: 0.4850\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 2.2247 Acc: 0.4123\n",
      "test Loss: 2.7296 Acc: 0.3410\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 2.2071 Acc: 0.4115\n",
      "test Loss: 2.1002 Acc: 0.4460\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 2.0182 Acc: 0.4663\n",
      "test Loss: 2.0890 Acc: 0.4620\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.8471 Acc: 0.5088\n",
      "test Loss: 1.5923 Acc: 0.5540\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.6757 Acc: 0.5573\n",
      "test Loss: 1.4828 Acc: 0.6040\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.5361 Acc: 0.5908\n",
      "test Loss: 1.5815 Acc: 0.5590\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.4334 Acc: 0.6153\n",
      "test Loss: 1.4431 Acc: 0.5930\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.3301 Acc: 0.6498\n",
      "test Loss: 1.3347 Acc: 0.6190\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.1736 Acc: 0.6845\n",
      "test Loss: 1.1286 Acc: 0.6770\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.1585 Acc: 0.6888\n",
      "test Loss: 0.9063 Acc: 0.7540\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.3100 Acc: 0.6445\n",
      "test Loss: 1.9711 Acc: 0.5210\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.3649 Acc: 0.6315\n",
      "test Loss: 1.2970 Acc: 0.6470\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.2938 Acc: 0.6503\n",
      "test Loss: 1.7211 Acc: 0.5500\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.2077 Acc: 0.6670\n",
      "test Loss: 1.1713 Acc: 0.6940\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.1569 Acc: 0.6950\n",
      "test Loss: 0.9711 Acc: 0.7380\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.0244 Acc: 0.7223\n",
      "test Loss: 1.3791 Acc: 0.6510\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.9633 Acc: 0.7398\n",
      "test Loss: 1.0500 Acc: 0.7260\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.8878 Acc: 0.7600\n",
      "test Loss: 1.0149 Acc: 0.7350\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.8353 Acc: 0.7695\n",
      "test Loss: 0.7559 Acc: 0.7930\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.7776 Acc: 0.7858\n",
      "test Loss: 0.7619 Acc: 0.8110\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.9625 Acc: 0.7375\n",
      "test Loss: 1.3594 Acc: 0.6440\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.1137 Acc: 0.6970\n",
      "test Loss: 1.5028 Acc: 0.6420\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.0495 Acc: 0.7135\n",
      "test Loss: 1.5726 Acc: 0.6210\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.9268 Acc: 0.7488\n",
      "test Loss: 1.0700 Acc: 0.7370\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.9018 Acc: 0.7508\n",
      "test Loss: 0.9350 Acc: 0.7490\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.9022 Acc: 0.7563\n",
      "test Loss: 0.9562 Acc: 0.7600\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.7651 Acc: 0.7928\n",
      "test Loss: 0.8019 Acc: 0.7880\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.7219 Acc: 0.8048\n",
      "test Loss: 0.8454 Acc: 0.7930\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.6767 Acc: 0.8190\n",
      "test Loss: 0.7053 Acc: 0.8270\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.6384 Acc: 0.8215\n",
      "test Loss: 0.8191 Acc: 0.8050\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.7992 Acc: 0.7760\n",
      "test Loss: 1.2581 Acc: 0.6980\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.8832 Acc: 0.7575\n",
      "test Loss: 1.0741 Acc: 0.7280\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.8695 Acc: 0.7633\n",
      "test Loss: 1.2596 Acc: 0.6990\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.8111 Acc: 0.7740\n",
      "test Loss: 0.8804 Acc: 0.7770\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.7241 Acc: 0.8045\n",
      "test Loss: 0.8637 Acc: 0.7850\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.6944 Acc: 0.8093\n",
      "test Loss: 0.8682 Acc: 0.7830\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.6668 Acc: 0.8135\n",
      "test Loss: 0.7559 Acc: 0.8160\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.6086 Acc: 0.8350\n",
      "test Loss: 0.7684 Acc: 0.8190\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.5843 Acc: 0.8433\n",
      "test Loss: 0.6322 Acc: 0.8450\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.5077 Acc: 0.8570\n",
      "test Loss: 0.6369 Acc: 0.8270\n",
      "\n",
      "Training complete in 9m 60s\n",
      "Best test Acc: 0.845000\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "trainer_B = Trainer(finetuner_B)\n",
    "fine_tuned_model_B, hist_B = trainer_B.fit(\n",
    "    dataloaders=dataloaders_dict,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    optimizer=optim.Adam,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "train_loss_hist_B, train_acc_hist_B, test_loss_hist_B, test_acc_hist_B = hist_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Case D: Freeze all the convolution blocks, introduce two FC layers prior to the softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "\t batch_norm.weight\n",
      "\t batch_norm.bias\n",
      "\t fc_2.weight\n",
      "\t fc_2.bias\n",
      "\t batch_norm_2.weight\n",
      "\t batch_norm_2.bias\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 4.8110 Acc: 0.0153\n",
      "test Loss: 4.6239 Acc: 0.0250\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 4.6297 Acc: 0.0273\n",
      "test Loss: 4.5325 Acc: 0.0540\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 4.5233 Acc: 0.0367\n",
      "test Loss: 4.4474 Acc: 0.0470\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 4.4347 Acc: 0.0480\n",
      "test Loss: 4.3844 Acc: 0.0740\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 4.3738 Acc: 0.0588\n",
      "test Loss: 4.3049 Acc: 0.0630\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 4.3222 Acc: 0.0638\n",
      "test Loss: 4.2392 Acc: 0.0750\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 4.2632 Acc: 0.0718\n",
      "test Loss: 4.1751 Acc: 0.0750\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 4.2155 Acc: 0.0828\n",
      "test Loss: 4.1104 Acc: 0.1040\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 4.1776 Acc: 0.0908\n",
      "test Loss: 4.0805 Acc: 0.1020\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 4.1345 Acc: 0.1020\n",
      "test Loss: 4.0615 Acc: 0.1080\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 4.1313 Acc: 0.0875\n",
      "test Loss: 4.1014 Acc: 0.0990\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 4.1669 Acc: 0.0895\n",
      "test Loss: 4.0127 Acc: 0.1070\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 4.1290 Acc: 0.1003\n",
      "test Loss: 4.0529 Acc: 0.1200\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 4.0686 Acc: 0.1070\n",
      "test Loss: 4.0071 Acc: 0.1130\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 4.0555 Acc: 0.1140\n",
      "test Loss: 3.9211 Acc: 0.1220\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 3.9706 Acc: 0.1223\n",
      "test Loss: 3.9145 Acc: 0.1130\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "trainer_C = Trainer(finetuner_C)\n",
    "fine_tuned_model_C, hist_C = trainer_C.fit(\n",
    "    dataloaders=dataloaders_dict,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    optimizer=optim.Adam,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "train_loss_hist_C, train_acc_hist_C, test_loss_hist_C, test_acc_hist_C = hist_C"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss_hist_base, label='Baseline (Train)')\n",
    "plt.plot(train_loss_hist_A, label='Model A (Train)')\n",
    "plt.plot(train_loss_hist_B, label='Model B (Train)')\n",
    "plt.plot(train_loss_hist_C, label='Model C (Train)')\n",
    "plt.plot(test_loss_hist_base, label='Baseline (Test)')\n",
    "plt.plot(test_loss_hist_A, label='Model A (Test)')\n",
    "plt.plot(test_loss_hist_B, label='Model B (Test)')\n",
    "plt.plot(test_loss_hist_C, label='Model C (Test)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(train_acc_hist_base, label='Baseline (Train)')\n",
    "plt.plot(train_acc_hist_A, label='Model A (Train)')\n",
    "plt.plot(train_acc_hist_B, label='Model B (Train)')\n",
    "plt.plot(train_acc_hist_C, label='Model C (Train)')\n",
    "plt.plot(test_acc_hist_base, label='Baseline (Test)')\n",
    "plt.plot(test_acc_hist_A, label='Model A (Test)')\n",
    "plt.plot(test_acc_hist_B, label='Model B (Test)')\n",
    "plt.plot(test_acc_hist_C, label='Model C (Test)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "783c5a5cff0fe5c79904b741d009b7c58ee2faf8ae465d9cae8287e59a309cd7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('dac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}